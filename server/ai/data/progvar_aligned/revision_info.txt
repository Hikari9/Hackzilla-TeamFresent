arguments: facenet/align/align_dataset_mtcnn.py data/progvar_raw data/progvar_aligned --gpu_memory_fraction 0.68
--------------------
git hash: b'6bdbcfa3bd38b4e8a13702099a7a31af2ba7e879'
--------------------
b'diff --git a/server/ai/Classifier Training + Prediction .ipynb b/server/ai/Classifier Training + Prediction .ipynb\ndeleted file mode 100644\nindex 5e9f6c6..0000000\n--- a/server/ai/Classifier Training + Prediction .ipynb\t\n+++ /dev/null\n@@ -1,166 +0,0 @@\n-{\n- "cells": [\n-  {\n-   "cell_type": "code",\n-   "execution_count": 1,\n-   "metadata": {\n-    "collapsed": true\n-   },\n-   "outputs": [],\n-   "source": [\n-    "import cv2, cv2.face\\n",\n-    "import os, os.path\\n",\n-    "import numpy as np"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 2,\n-   "metadata": {\n-    "collapsed": false\n-   },\n-   "outputs": [],\n-   "source": [\n-    "import train"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 3,\n-   "metadata": {\n-    "collapsed": false,\n-    "scrolled": true\n-   },\n-   "outputs": [\n-    {\n-     "name": "stdout",\n-     "output_type": "stream",\n-     "text": [\n-      "Accuracy: 100.00000%\\n",\n-      "Collected 75 images [10 labels]\\n",\n-      "Saved classifier /home/rico/Git/Fresent/server/ai/classifiers/Charles_Taylor.xml\\n",\n-      "Saved classifier /home/rico/Git/Fresent/server/ai/classifiers/Jesse_Jackson.xml\\n",\n-      "Saved classifier /home/rico/Git/Fresent/server/ai/classifiers/Hosni_Mubarak.xml\\n",\n-      "Saved classifier /home/rico/Git/Fresent/server/ai/classifiers/George_Clooney.xml\\n",\n-      "Saved classifier /home/rico/Git/Fresent/server/ai/classifiers/Hugh_Grant.xml\\n",\n-      "Saved classifier /home/rico/Git/Fresent/server/ai/classifiers/Jeong_Se-hyun.xml\\n",\n-      "Saved classifier /home/rico/Git/Fresent/server/ai/classifiers/Heizo_Takenaka.xml\\n",\n-      "Saved classifier /home/rico/Git/Fresent/server/ai/classifiers/Aaron_Peirsol.xml\\n",\n-      "Saved classifier /home/rico/Git/Fresent/server/ai/classifiers/Fernando_Gonzalez.xml\\n",\n-      "Saved classifier /home/rico/Git/Fresent/server/ai/classifiers/Colin_Farrell.xml\\n"\n-     ]\n-    }\n-   ],\n-   "source": [\n-    "train.generate_classifiers(\'data/sample_training\', \'classifiers\', None)"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 4,\n-   "metadata": {\n-    "collapsed": false\n-   },\n-   "outputs": [\n-    {\n-     "name": "stdout",\n-     "output_type": "stream",\n-     "text": [\n-      "(True, 70.33203017168863)\\n",\n-      "(False, 64.02631180994958)\\n"\n-     ]\n-    }\n-   ],\n-   "source": [\n-    "# Test matching with some test data\\n",\n-    "\\n",\n-    "import predict, util\\n",\n-    "\\n",\n-    "hosni = util.load_grayscale_image(\'data/sample_testing/Hosni_Mubarak/Hosni_Mubarak_0009.png\')\\n",\n-    "aaron = util.load_grayscale_image(\'data/sample_testing/Aaron_Peirsol/Aaron_Peirsol_0004.png\')\\n",\n-    "\\n",\n-    "import cv2\\n",\n-    "cv2.imshow(\'Hello\', aaron)\\n",\n-    "\\n",\n-    "print(predict.match_classifier(\'classifiers/Hosni_Mubarak.xml\', hosni))\\n",\n-    "print(predict.match_classifier(\'classifiers/Hosni_Mubarak.xml\', aaron))"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 7,\n-   "metadata": {\n-    "collapsed": false\n-   },\n-   "outputs": [\n-    {\n-     "name": "stdout",\n-     "output_type": "stream",\n-     "text": [\n-      "Accuracy: 100.00000%\\n",\n-      "Expected=True (True, 70.33203017168863)\\n",\n-      "Expected=True (False, 66.9695790300998)\\n",\n-      "Expected=True (False, 64.02631180994958)\\n",\n-      "Expected=True (True, 62.252336352937334)\\n",\n-      "Expected=True (False, 55.98307131663556)\\n",\n-      "Expected=True (True, 58.443145513715066)\\n",\n-      "Expected=True (True, 58.194282662587064)\\n",\n-      "Expected=True (False, 61.83141922223049)\\n",\n-      "Expected=True (True, 60.659852760896726)\\n",\n-      "Expected=True (True, 69.14819846258747)\\n"\n-     ]\n-    }\n-   ],\n-   "source": [\n-    "# test all labels\\n",\n-    "import util\\n",\n-    "images, labels = util.get_images_and_labels(\'data/sample_testing\')\\n",\n-    "\\n",\n-    "correct = 0\\n",\n-    "total = 0\\n",\n-    "\\n",\n-    "for image, label in zip(images, labels):\\n",\n-    "    \\n",\n-    "    is_match, conf = predict.match_classifier(\'classifiers/\' + label + \'.xml\', image)\\n",\n-    "    print(\'Match:\', is_match, conf)\\n",\n-    "    \\n",\n-    "    if is_match:\\n",\n-    "        correct += 1\\n",\n-    "    total += 1\\n",\n-    "\\n",\n-    "print(\'Accuracy: %.5f\' % (100 * correct / max(1, total)))\\n",\n-    "    "\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": null,\n-   "metadata": {\n-    "collapsed": true\n-   },\n-   "outputs": [],\n-   "source": []\n-  }\n- ],\n- "metadata": {\n-  "kernelspec": {\n-   "display_name": "Tensorflow",\n-   "language": "python",\n-   "name": "tensorflow"\n-  },\n-  "language_info": {\n-   "codemirror_mode": {\n-    "name": "ipython",\n-    "version": 3\n-   },\n-   "file_extension": ".py",\n-   "mimetype": "text/x-python",\n-   "name": "python",\n-   "nbconvert_exporter": "python",\n-   "pygments_lexer": "ipython3",\n-   "version": "3.5.2"\n-  }\n- },\n- "nbformat": 4,\n- "nbformat_minor": 2\n-}\ndiff --git a/server/ai/Face Recognition - Classifier Comparison.ipynb b/server/ai/Face Recognition - Classifier Comparison.ipynb\ndeleted file mode 100644\nindex 1faf78c..0000000\n--- a/server/ai/Face Recognition - Classifier Comparison.ipynb\t\n+++ /dev/null\n@@ -1,619 +0,0 @@\n-{\n- "cells": [\n-  {\n-   "cell_type": "markdown",\n-   "metadata": {},\n-   "source": [\n-    "# Face Recognition - Generate Eigenface Classifiers\\n",\n-    "\\n",\n-    "This notebook tests face detection and recognition from sample images"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 1,\n-   "metadata": {\n-    "collapsed": true\n-   },\n-   "outputs": [],\n-   "source": [\n-    "import cv2, cv2.face\\n",\n-    "import os, os.path\\n",\n-    "import numpy as np\\n",\n-    "from PIL import Image"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 2,\n-   "metadata": {\n-    "collapsed": true\n-   },\n-   "outputs": [],\n-   "source": [\n-    "# constants\\n",\n-    "CASCADE_PATH = \'./data/\'\\n",\n-    "CASCADE_NAME = \'haarcascade_frontalface_default.xml\'\\n",\n-    "SAMPLE_TRAINING = \'./data/sample_training/\'\\n",\n-    "SAMPLE_TESTING = \'./data/sample_testing/\'"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 3,\n-   "metadata": {\n-    "collapsed": true\n-   },\n-   "outputs": [],\n-   "source": [\n-    "#vSAMPLE_TRAINING = \'/home/rico/datasets/lfw/raw/\'"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 4,\n-   "metadata": {\n-    "collapsed": false\n-   },\n-   "outputs": [],\n-   "source": [\n-    "# Viola-Jones classifier for Haar feature extraction\\n",\n-    "cascader_file = os.path.join(CASCADE_PATH, CASCADE_NAME)\\n",\n-    "cascader = cv2.CascadeClassifier(cascader_file)\\n",\n-    "assert(not cascader.empty())\\n",\n-    "\\n",\n-    "cascader_kwargs = { # setup detection args here\\n",\n-    "    \'scaleFactor\': 1.1,\\n",\n-    "    \'minNeighbors\': 2,\\n",\n-    "    \'flags\': 2\\n",\n-    "}"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 5,\n-   "metadata": {\n-    "collapsed": true\n-   },\n-   "outputs": [],\n-   "source": [\n-    "from util import get_images_and_labels"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 6,\n-   "metadata": {\n-    "collapsed": false\n-   },\n-   "outputs": [\n-    {\n-     "name": "stdout",\n-     "output_type": "stream",\n-     "text": [\n-      "Hosni_Mubarak\\n",\n-      "\\tHosni_Mubarak_0007.png [1 face]\\n",\n-      "\\tHosni_Mubarak_0003.png [1 face]\\n",\n-      "\\tHosni_Mubarak_0004.png [1 face]\\n",\n-      "\\tHosni_Mubarak_0008.png [1 face]\\n",\n-      "\\tHosni_Mubarak_0005.png [1 face]\\n",\n-      "\\tHosni_Mubarak_0001.png [1 face]\\n",\n-      "\\tHosni_Mubarak_0006.png [1 face]\\n",\n-      "\\tHosni_Mubarak_0002.png [1 face]\\n",\n-      "[Added 8 images]\\n",\n-      "Jeong_Se-hyun\\n",\n-      "\\tJeong_Se-hyun_0007.png [1 face]\\n",\n-      "\\tJeong_Se-hyun_0001.png [1 face]\\n",\n-      "\\tJeong_Se-hyun_0008.png [1 face]\\n",\n-      "\\tJeong_Se-hyun_0003.png [1 face]\\n",\n-      "\\tJeong_Se-hyun_0004.png [1 face]\\n",\n-      "\\tJeong_Se-hyun_0002.png [1 face]\\n",\n-      "\\tJeong_Se-hyun_0005.png [1 face]\\n",\n-      "\\tJeong_Se-hyun_0006.png [1 face]\\n",\n-      "[Added 16 images]\\n",\n-      "Aaron_Peirsol\\n",\n-      "\\tAaron_Peirsol_0003.png [1 face]\\n",\n-      "\\tAaron_Peirsol_0001.png [1 face]\\n",\n-      "\\tAaron_Peirsol_0002.png [1 face]\\n",\n-      "[Added 19 images]\\n",\n-      "Heizo_Takenaka\\n",\n-      "\\tHeizo_Takenaka_0002.png [1 face]\\n",\n-      "\\tHeizo_Takenaka_0007.png [1 face]\\n",\n-      "\\tHeizo_Takenaka_0001.png [1 face]\\n",\n-      "\\tHeizo_Takenaka_0006.png [1 face]\\n",\n-      "\\tHeizo_Takenaka_0004.png [1 face]\\n",\n-      "\\tHeizo_Takenaka_0008.png [1 face]\\n",\n-      "\\tHeizo_Takenaka_0003.png [1 face]\\n",\n-      "\\tHeizo_Takenaka_0005.png [1 face]\\n",\n-      "[Added 27 images]\\n",\n-      "Jesse_Jackson\\n",\n-      "\\tJesse_Jackson_0001.png [1 face]\\n",\n-      "\\tJesse_Jackson_0007.png [1 face]\\n",\n-      "\\tJesse_Jackson_0004.png [1 face]\\n",\n-      "\\tJesse_Jackson_0002.png [1 face]\\n",\n-      "\\tJesse_Jackson_0003.png [1 face]\\n",\n-      "\\tJesse_Jackson_0008.png [1 face]\\n",\n-      "\\tJesse_Jackson_0006.png [1 face]\\n",\n-      "\\tJesse_Jackson_0005.png [1 face]\\n",\n-      "[Added 35 images]\\n",\n-      "Hugh_Grant\\n",\n-      "\\tHugh_Grant_0002.png [1 face]\\n",\n-      "\\tHugh_Grant_0006.png [1 face]\\n",\n-      "\\tHugh_Grant_0004.png [1 face]\\n",\n-      "\\tHugh_Grant_0008.png [1 face]\\n",\n-      "\\tHugh_Grant_0001.png [1 face]\\n",\n-      "\\tHugh_Grant_0007.png [1 face]\\n",\n-      "\\tHugh_Grant_0003.png [1 face]\\n",\n-      "\\tHugh_Grant_0005.png [1 face]\\n",\n-      "[Added 43 images]\\n",\n-      "Fernando_Gonzalez\\n",\n-      "\\tFernando_Gonzalez_0006.png [1 face]\\n",\n-      "\\tFernando_Gonzalez_0007.png [1 face]\\n",\n-      "\\tFernando_Gonzalez_0004.png [1 face]\\n",\n-      "\\tFernando_Gonzalez_0002.png [1 face]\\n",\n-      "\\tFernando_Gonzalez_0003.png [1 face]\\n",\n-      "\\tFernando_Gonzalez_0001.png [1 face]\\n",\n-      "\\tFernando_Gonzalez_0005.png [1 face]\\n",\n-      "\\tFernando_Gonzalez_0008.png [1 face]\\n",\n-      "[Added 51 images]\\n",\n-      "George_Clooney\\n",\n-      "\\tGeorge_Clooney_0007.png [1 face]\\n",\n-      "\\tGeorge_Clooney_0001.png [1 face]\\n",\n-      "\\tGeorge_Clooney_0003.png [1 face]\\n",\n-      "\\tGeorge_Clooney_0006.png [1 face]\\n",\n-      "\\tGeorge_Clooney_0005.png [1 face]\\n",\n-      "\\tGeorge_Clooney_0008.png [1 face]\\n",\n-      "\\tGeorge_Clooney_0004.png [1 face]\\n",\n-      "\\tGeorge_Clooney_0002.png [1 face]\\n",\n-      "[Added 59 images]\\n",\n-      "Colin_Farrell\\n",\n-      "\\tColin_Farrell_0007.png [1 face]\\n",\n-      "\\tColin_Farrell_0002.png [1 face]\\n",\n-      "\\tColin_Farrell_0006.png [1 face]\\n",\n-      "\\tColin_Farrell_0003.png [1 face]\\n",\n-      "\\tColin_Farrell_0004.png [1 face]\\n",\n-      "\\tColin_Farrell_0008.png [1 face]\\n",\n-      "\\tColin_Farrell_0005.png [1 face]\\n",\n-      "\\tColin_Farrell_0001.png [1 face]\\n",\n-      "[Added 67 images]\\n",\n-      "Charles_Taylor\\n",\n-      "\\tCharles_Taylor_0002.png [1 face]\\n",\n-      "\\tCharles_Taylor_0005.png [1 face]\\n",\n-      "\\tCharles_Taylor_0004.png [1 face]\\n",\n-      "\\tCharles_Taylor_0001.png [1 face]\\n",\n-      "\\tCharles_Taylor_0007.png [1 face]\\n",\n-      "\\tCharles_Taylor_0003.png [1 face]\\n",\n-      "\\tCharles_Taylor_0006.png [1 face]\\n",\n-      "\\tCharles_Taylor_0008.png [1 face]\\n",\n-      "[Added 75 images]\\n"\n-     ]\n-    }\n-   ],\n-   "source": [\n-    "# setup training set\\n",\n-    "training_set = get_images_and_labels(SAMPLE_TRAINING,\\n",\n-    "                                    cascader=None,\\n",\n-    "                                    debug=True,\\n",\n-    "                                    debug_accuracy=False)"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 7,\n-   "metadata": {\n-    "collapsed": false\n-   },\n-   "outputs": [\n-    {\n-     "name": "stdout",\n-     "output_type": "stream",\n-     "text": [\n-      "Accuracy: 100.00000%\\n",\n-      "Accuracy: 100.00000%\\n"\n-     ]\n-    }\n-   ],\n-   "source": [\n-    "# setup testing set\\n",\n-    "testing_set_cascade = get_images_and_labels(SAMPLE_TESTING,\\n",\n-    "                                    cascader=cascader,\\n",\n-    "                                    debug=False,\\n",\n-    "                                    debug_faces=True,\\n",\n-    "                                    debug_accuracy=True)\\n",\n-    "\\n",\n-    "testing_set= get_images_and_labels(SAMPLE_TESTING,\\n",\n-    "                                    cascader=None,\\n",\n-    "                                    debug=False,\\n",\n-    "                                    debug_faces=True,\\n",\n-    "                                    debug_accuracy=True)\\n"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 8,\n-   "metadata": {\n-    "collapsed": false\n-   },\n-   "outputs": [],\n-   "source": [\n-    "# setup labels\\n",\n-    "label_dict = {}\\n",\n-    "label_dict_inv = {}\\n",\n-    "\\n",\n-    "for label in training_set[1]:\\n",\n-    "    if label not in label_dict:\\n",\n-    "        label_dict_inv[len(label_dict)] = label\\n",\n-    "        label_dict[label] = len(label_dict)\\n",\n-    "        "\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 9,\n-   "metadata": {\n-    "collapsed": false\n-   },\n-   "outputs": [],\n-   "source": [\n-    "def perform_training(recognizer):\\n",\n-    "    images, raw_labels = training_set\\n",\n-    "    labels = [label_dict[label] for label in raw_labels]\\n",\n-    "    print(labels)\\n",\n-    "    recognizer.train(images, np.array(labels))\\n",\n-    "    print(\'Trained %d images\\\\n\' % len(images))"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 10,\n-   "metadata": {\n-    "collapsed": false\n-   },\n-   "outputs": [],\n-   "source": [\n-    "def perform_testing(recognizer, testing_set=testing_set):\\n",\n-    "    \\n",\n-    "    images, labels = testing_set\\n",\n-    "    \\n",\n-    "    print(\'Performing %d tests\\\\n\' % len(images))\\n",\n-    "\\n",\n-    "    correct = 0\\n",\n-    "    total = 0\\n",\n-    "\\n",\n-    "    for image, expected in zip(images, labels):\\n",\n-    "        \\n",\n-    "        if expected not in label_dict:\\n",\n-    "            print(\'No face found for %s\' % expected)\\n",\n-    "            \\n",\n-    "        else:\\n",\n-    "            eid = label_dict[expected]\\n",\n-    "            lid, confidence = recognizer.predict(image)\\n",\n-    "            \\n",\n-    "            if lid in label_dict_inv:\\n",\n-    "                \\n",\n-    "                actual = label_dict_inv[lid]\\n",\n-    "                                        \\n",\n-    "                if eid == lid:\\n",\n-    "                    print(\'%s is correctly recognized with confidence %.10f\' % (expected, confidence))\\n",\n-    "                    correct += 1\\n",\n-    "                                        \\n",\n-    "                else:\\n",\n-    "                    print(\'%s incorrect (recognized as %s with confidence %.10f)\' % (expected, actual, confidence))\\n",\n-    "                    \\n",\n-    "            else:\\n",\n-    "                print(\'No face found for %s\' % expected)\\n",\n-    "                \\n",\n-    "        total += 1\\n",\n-    "\\n",\n-    "    print()\\n",\n-    "    print(\'Accuracy: %.2f%%\' % (100 * (correct / max(1, total))))\\n",\n-    "    print(\'Correct:  %d\' % correct)\\n",\n-    "    print(\'Wrong:    %d\' % (total - correct))"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 11,\n-   "metadata": {\n-    "collapsed": false\n-   },\n-   "outputs": [\n-    {\n-     "name": "stdout",\n-     "output_type": "stream",\n-     "text": [\n-      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9]\\n",\n-      "Trained 75 images\\n",\n-      "\\n",\n-      "0 0 70.33203017168863\\n",\n-      "1 4 66.9695790300998\\n",\n-      "2 8 64.02631180994958\\n",\n-      "3 3 62.252336352937334\\n",\n-      "4 5 55.98307131663556\\n",\n-      "5 5 58.443145513715066\\n",\n-      "6 6 58.194282662587064\\n",\n-      "7 1 61.83141922223049\\n",\n-      "8 8 60.659852760896726\\n",\n-      "9 9 69.14819846258747\\n",\n-      "60.0 % Accuracy\\n"\n-     ]\n-    }\n-   ],\n-   "source": [\n-    "reco = cv2.face.createLBPHFaceRecognizer()\\n",\n-    "perform_training(reco)\\n",\n-    "ims, ls = testing_set\\n",\n-    "cor = 0\\n",\n-    "tot = 0\\n",\n-    "for im, l in zip(ims,ls):\\n",\n-    "    label = label_dict[l]\\n",\n-    "    inf, conf = reco.predict(im)\\n",\n-    "    print(label, inf, conf)\\n",\n-    "    if (label == inf):\\n",\n-    "        cor += 1\\n",\n-    "    tot += 1\\n",\n-    "print(100*cor/tot,\'% Accuracy\')"\n-   ]\n-  },\n-  {\n-   "cell_type": "markdown",\n-   "metadata": {},\n-   "source": [\n-    "# 1. EigenFaces"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 12,\n-   "metadata": {\n-    "collapsed": false\n-   },\n-   "outputs": [],\n-   "source": [\n-    "# face recognizer\\n",\n-    "recognizer = cv2.face.createEigenFaceRecognizer()"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 13,\n-   "metadata": {\n-    "collapsed": false,\n-    "scrolled": true\n-   },\n-   "outputs": [\n-    {\n-     "name": "stdout",\n-     "output_type": "stream",\n-     "text": [\n-      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9]\\n",\n-      "Trained 75 images\\n",\n-      "\\n",\n-      "CPU times: user 532 ms, sys: 48 ms, total: 580 ms\\n",\n-      "Wall time: 579 ms\\n"\n-     ]\n-    }\n-   ],\n-   "source": [\n-    "%%time\\n",\n-    "# Perform the training\\n",\n-    "perform_training(recognizer)"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 14,\n-   "metadata": {\n-    "collapsed": false\n-   },\n-   "outputs": [\n-    {\n-     "name": "stdout",\n-     "output_type": "stream",\n-     "text": [\n-      "Performing 10 tests\\n",\n-      "\\n",\n-      "Hosni_Mubarak incorrect (recognized as Jeong_Se-hyun with confidence 7708.8797264715)\\n",\n-      "Jeong_Se-hyun incorrect (recognized as Jesse_Jackson with confidence 9191.3998485021)\\n",\n-      "Aaron_Peirsol incorrect (recognized as Jeong_Se-hyun with confidence 7686.4538475870)\\n",\n-      "Heizo_Takenaka is correctly recognized with confidence 7592.1110518190\\n",\n-      "Jesse_Jackson incorrect (recognized as George_Clooney with confidence 7535.3358054781)\\n",\n-      "Hugh_Grant is correctly recognized with confidence 7144.1526959103\\n",\n-      "Fernando_Gonzalez is correctly recognized with confidence 7184.3745207685\\n",\n-      "George_Clooney is correctly recognized with confidence 8396.7570350231\\n",\n-      "Colin_Farrell incorrect (recognized as Hugh_Grant with confidence 7856.0370864173)\\n",\n-      "Charles_Taylor incorrect (recognized as Fernando_Gonzalez with confidence 9496.5626098884)\\n",\n-      "\\n",\n-      "Accuracy: 40.00%\\n",\n-      "Correct:  4\\n",\n-      "Wrong:    6\\n"\n-     ]\n-    }\n-   ],\n-   "source": [\n-    "perform_testing(recognizer)"\n-   ]\n-  },\n-  {\n-   "cell_type": "markdown",\n-   "metadata": {},\n-   "source": [\n-    "# 2. FisherFaces"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 15,\n-   "metadata": {\n-    "collapsed": true\n-   },\n-   "outputs": [],\n-   "source": [\n-    "# face recognizer\\n",\n-    "recognizer = cv2.face.createFisherFaceRecognizer()"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 16,\n-   "metadata": {\n-    "collapsed": false\n-   },\n-   "outputs": [\n-    {\n-     "name": "stdout",\n-     "output_type": "stream",\n-     "text": [\n-      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9]\\n",\n-      "Trained 75 images\\n",\n-      "\\n",\n-      "CPU times: user 492 ms, sys: 16 ms, total: 508 ms\\n",\n-      "Wall time: 506 ms\\n"\n-     ]\n-    }\n-   ],\n-   "source": [\n-    "%%time\\n",\n-    "perform_training(recognizer)"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 17,\n-   "metadata": {\n-    "collapsed": false\n-   },\n-   "outputs": [\n-    {\n-     "name": "stdout",\n-     "output_type": "stream",\n-     "text": [\n-      "Performing 10 tests\\n",\n-      "\\n",\n-      "Hosni_Mubarak incorrect (recognized as George_Clooney with confidence 1415.5985222224)\\n",\n-      "Jeong_Se-hyun incorrect (recognized as Colin_Farrell with confidence 1941.4372785408)\\n",\n-      "Aaron_Peirsol incorrect (recognized as Hosni_Mubarak with confidence 1495.4150848578)\\n",\n-      "Heizo_Takenaka incorrect (recognized as Colin_Farrell with confidence 2170.6659331031)\\n",\n-      "Jesse_Jackson incorrect (recognized as Hugh_Grant with confidence 1476.5334439179)\\n",\n-      "Hugh_Grant is correctly recognized with confidence 1683.8350626277\\n",\n-      "Fernando_Gonzalez is correctly recognized with confidence 1736.9916732560\\n",\n-      "George_Clooney is correctly recognized with confidence 1638.8801908796\\n",\n-      "Colin_Farrell incorrect (recognized as Fernando_Gonzalez with confidence 1593.3159287082)\\n",\n-      "Charles_Taylor incorrect (recognized as Hosni_Mubarak with confidence 1666.5036180065)\\n",\n-      "\\n",\n-      "Accuracy: 30.00%\\n",\n-      "Correct:  3\\n",\n-      "Wrong:    7\\n",\n-      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\\n",\n-      "Wall time: 5.25 ms\\n"\n-     ]\n-    }\n-   ],\n-   "source": [\n-    "%%time\\n",\n-    "perform_testing(recognizer)"\n-   ]\n-  },\n-  {\n-   "cell_type": "markdown",\n-   "metadata": {},\n-   "source": [\n-    "# 2. Local Binary Patterns"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 18,\n-   "metadata": {\n-    "collapsed": true\n-   },\n-   "outputs": [],\n-   "source": [\n-    "# face recognizer\\n",\n-    "recognizer = cv2.face.createLBPHFaceRecognizer()"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 19,\n-   "metadata": {\n-    "collapsed": false\n-   },\n-   "outputs": [\n-    {\n-     "name": "stdout",\n-     "output_type": "stream",\n-     "text": [\n-      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9]\\n",\n-      "Trained 75 images\\n",\n-      "\\n",\n-      "CPU times: user 204 ms, sys: 0 ns, total: 204 ms\\n",\n-      "Wall time: 198 ms\\n"\n-     ]\n-    }\n-   ],\n-   "source": [\n-    "%%time\\n",\n-    "perform_training(recognizer)"\n-   ]\n-  },\n-  {\n-   "cell_type": "code",\n-   "execution_count": 20,\n-   "metadata": {\n-    "collapsed": false\n-   },\n-   "outputs": [\n-    {\n-     "name": "stdout",\n-     "output_type": "stream",\n-     "text": [\n-      "Performing 10 tests\\n",\n-      "\\n",\n-      "Hosni_Mubarak is correctly recognized with confidence 70.3320301717\\n",\n-      "Jeong_Se-hyun incorrect (recognized as Jesse_Jackson with confidence 66.9695790301)\\n",\n-      "Aaron_Peirsol incorrect (recognized as Colin_Farrell with confidence 64.0263118099)\\n",\n-      "Heizo_Takenaka is correctly recognized with confidence 62.2523363529\\n",\n-      "Jesse_Jackson incorrect (recognized as Hugh_Grant with confidence 55.9830713166)\\n",\n-      "Hugh_Grant is correctly recognized with confidence 58.4431455137\\n",\n-      "Fernando_Gonzalez is correctly recognized with confidence 58.1942826626\\n",\n-      "George_Clooney incorrect (recognized as Jeong_Se-hyun with confidence 61.8314192222)\\n",\n-      "Colin_Farrell is correctly recognized with confidence 60.6598527609\\n",\n-      "Charles_Taylor is correctly recognized with confidence 69.1481984626\\n",\n-      "\\n",\n-      "Accuracy: 60.00%\\n",\n-      "Correct:  6\\n",\n-      "Wrong:    4\\n",\n-      "CPU times: user 88 ms, sys: 0 ns, total: 88 ms\\n",\n-      "Wall time: 85.1 ms\\n"\n-     ]\n-    }\n-   ],\n-   "source": [\n-    "%%time\\n",\n-    "perform_testing(recognizer, testing_set)"\n-   ]\n-  }\n- ],\n- "metadata": {\n-  "kernelspec": {\n-   "display_name": "Tensorflow",\n-   "language": "python",\n-   "name": "tensorflow"\n-  },\n-  "language_info": {\n-   "codemirror_mode": {\n-    "name": "ipython",\n-    "version": 3\n-   },\n-   "file_extension": ".py",\n-   "mimetype": "text/x-python",\n-   "name": "python",\n-   "nbconvert_exporter": "python",\n-   "pygments_lexer": "ipython3",\n-   "version": "3.5.2"\n-  }\n- },\n- "nbformat": 4,\n- "nbformat_minor": 2\n-}\ndiff --git a/server/ai/predict.py b/server/ai/predict.py\ndeleted file mode 100644\nindex f839e0c..0000000\n--- a/server/ai/predict.py\n+++ /dev/null\n@@ -1,25 +0,0 @@\n-import cv2, cv2.face\n-import os, os.path\n-import sys\n-import numpy as np\n-import train\n-from util import get_images_and_labels\n-\n-# Returns a boolean if an image matches with classifier, attach with confidence\n-# -> is_match, confidence\n-def match_classifier(classifer_file,\n-                     image,\n-                     algorithm=train.ALGO_LOCAL_BINARY_PATTERNS):\n-\n-    # create recognizer\n-    FaceRecognizer = algorithm\n-    recognizer = FaceRecognizer()\n-\n-    # load recognizer from classifier file\n-    recognizer.load(classifer_file)\n-\n-    # perform prediction and confidence\n-    label, confidence = recognizer.predict(image)\n-    label = True if label == 1 else False\n-\n-    return label, confidence\ndiff --git a/server/ai/train.py b/server/ai/train.py\ndeleted file mode 100644\nindex d7e401e..0000000\n--- a/server/ai/train.py\n+++ /dev/null\n@@ -1,78 +0,0 @@\n-import cv2, cv2.face\n-import os, os.path\n-import sys\n-import numpy as np\n-from util import get_images_and_labels\n-\n-# constants\n-CASCADE_NAME = \'haarcascade_frontalface_default.xml\'\n-ALGO_LOCAL_BINARY_PATTERNS = cv2.face.createLBPHFaceRecognizer\n-ALGO_EIGEN = cv2.face.createEigenFaceRecognizer\n-ALGO_FISHER = cv2.face.createFisherFaceRecognizer\n-\n-# perform training and generate classifiers\n-def generate_classifiers(training_folder,\n-                         classifier_folder,\n-                         cascade_folder,\n-                         algorithm=ALGO_LOCAL_BINARY_PATTERNS):\n-\n-    # Check if classifier folder exists\n-    if not os.path.exists(classifier_folder):\n-        print(\'Creating folder `%s`\' % classifier_folder)\n-        os.makedirs(classifier_folder)\n-\n-    # Viola-Jones classifier for Haar feature extraction\n-    cascader = None\n-    if cascade_folder is not None:\n-        cascade_path = os.path.join(cascade_folder, CASCADE_NAME)\n-        cascader = cv2.CascadeClassifier(cascade_path)\n-        assert(not cascader.empty())\n-\n-    # Setup cascader args\n-    cascader_args = {\n-        \'scaleFactor\': 1.1,\n-        \'minNeighbors\': 2,\n-        \'flags\': 2\n-    }\n-\n-    # Get images and labels\n-    images, labels = get_images_and_labels(training_folder,\n-                                           cascader=cascader,\n-                                           cascader_args=cascader_args,\n-                                           debug_accuracy=True)\n-\n-    print(\'Collected %d images [%d labels]\' % (len(images), len(set(labels))))\n-\n-    FaceRecognizer = algorithm\n-\n-    label_dict = {label: 0 for label in labels}\n-    index = 2\n-    for label in label_dict:\n-        label_dict[label] = index\n-        index += 1\n-\n-    for label in label_dict:\n-\n-        recognizer = FaceRecognizer()\n-\n-        # create a binary recognizer per image\n-        # current complexity: O(n^2)\n-        # TODO: optimize this and integrate to TensorFlow\n-        recognizer.train(images, np.array([1 if label == cur_label else 0 for image, cur_label in zip(images, labels)]))\n-        # recognizer.train(images, np.array([\n-            # 1 if label == cur_label else label_dict[label]\n-            # for cur_label in labels]))\n-\n-        # Save classifier to text file\n-        file_name = label + \'.xml\'\n-        file_path = os.path.join(classifier_folder, file_name)\n-\n-        recognizer.save(file_path)\n-        print(\'Saved classifier \' + os.path.abspath(file_path))\n-\n-if __name__ == \'__main__\':\n-    args = sys.argv\n-    if len(args) < 3:\n-        print(\'Usage: python %s <training_folder> <classifier_folder> [<cascade_folder>]\' % args[0])\n-    else:\n-        generate_classifiers(args[1], args[2], \'../data/\' if len(args) == 3 else args[3])\ndiff --git a/server/ai/util.py b/server/ai/util.py\ndeleted file mode 100644\nindex 1fdb592..0000000\n--- a/server/ai/util.py\n+++ /dev/null\n@@ -1,84 +0,0 @@\n-import cv2, cv2.face\n-import os, os.path\n-import sys\n-import numpy as np\n-def load_grayscale_image(image_file,\n-                         equalize_hist=True,\n-                         median_blur=3):\n-\n-    raw_image = cv2.imread(image_file)\n-    gray_image = cv2.cvtColor(raw_image, cv2.COLOR_BGR2GRAY)\n-    image = gray_image\n-\n-    if equalize_hist:\n-        test = cv2.equalizeHist(image)\n-\n-    if median_blur:\n-        image = cv2.medianBlur(image, median_blur)\n-\n-    return image\n-\n-# function that gets images with respective labels from a given folder\n-def get_images_and_labels(folder,\n-                          cascader=None,\n-                          cascader_args={},\n-                          debug=False,\n-                          debug_faces=False,\n-                          debug_accuracy=False,\n-                          **kwargs):\n-\n-    labels = []\n-    images = []\n-    added = 0\n-    expected = 0\n-\n-    for label in os.listdir(folder):\n-\n-        if debug: print(label)\n-        path = os.path.join(folder, label)\n-\n-        if os.path.isdir(path):\n-\n-            for filename in os.listdir(path):\n-\n-                ok = 0\n-                expected += 1\n-\n-                try:\n-\n-                    image_path = os.path.join(path, filename)\n-                    image = load_grayscale_image(image_path, **kwargs)\n-\n-                    if cascader is not None:\n-                        # get the face using Viola-Jones detector\n-                        faces = cascader.detectMultiScale(image, **cascader_args)\n-                        for (x, y, w, h) in faces:\n-                            images.append(image[y:y+h, x:x+w])\n-                            labels.append(label)\n-                            if debug_faces:\n-                                cv2.imshow(\'Adding faces to training set...\', image[y:y+h, x:x+w])\n-                                cv2.waitKey(50)\n-                        ok += len(faces)\n-                        if len(faces) > 0:\n-                            added += 1\n-                    else:\n-                        images.append(image)\n-                        labels.append(label)\n-                        ok = 1\n-                        added += 1\n-\n-                except Exception as e:\n-                    print(e)\n-                    pass\n-\n-                if debug: print(\'\\t\' + filename + (\' [%d face]\' % ok))\n-\n-        if debug: print(\'[Added %d images]\' % added)\n-\n-    if debug_faces:\n-        cv2.destroyAllWindows()\n-\n-    if debug_accuracy:\n-        print(\'Accuracy: %.5f%%\' % (100 * added / max(1, expected)))\n-\n-    return images, labels'